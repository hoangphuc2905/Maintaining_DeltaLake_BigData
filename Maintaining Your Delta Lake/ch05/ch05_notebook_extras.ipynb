{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea3dbe2-49af-4c3a-8cba-bb41dfe89521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run first for imports\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import col, desc\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38b60d30-a820-4125-b4d5-c20415e1b23b",
   "metadata": {},
   "source": [
    "# Chapter 5: Maintaining your Delta Lake (Extras)\n",
    "> The following exercises use the open-source [nyc_taxi dataset](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page), specifically the Yellow Taxi Trip Records parquet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31e57b3-1810-433e-91b1-47654ae9db38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "     CREATE TABLE IF NOT EXISTS default.nyc_taxi (\n",
    "       VendorID BIGINT\n",
    "     ) USING DELTA\n",
    "     TBLPROPERTIES('delta.logRetentionDuration'='interval 7 days');\n",
    "   \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff8175c-3a5a-4beb-b2c7-dd0189aef47c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad93a88e-acef-42db-9547-d7dae2bd359d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark.sql(\"drop table default.nyc_taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ac91e5-37fe-4772-95a4-634a67da7f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# should be zero on your first pass\n",
    "# this means that while the table exists, there are no data files associated with it\n",
    "len(spark.table(\"default.nyc_taxi\").inputFiles())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1aa0af8-06c6-40d4-b659-afc8d20c0467",
   "metadata": {},
   "source": [
    "## Start Populating the Table\n",
    "> The next three commands are used to show Schema Evolution and Validation with Delta Lake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de164e96-5ef7-4385-9e23-be8d6672cfa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Populate the Table reading the Parquet nyc_taxi Data\n",
    "# note: this will fail and that is okay\n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/data/datasets/nyc_taxi/yellow_tripdata_2023-01.parquet\")\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .saveAsTable(\"default.nyc_taxi\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba42d257-59e3-46aa-8f4e-dafb821a82f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# one step closer, there is still something missing...\n",
    "# and yes, this operation still fails... if only...\n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/data/datasets/nyc_taxi/yellow_tripdata_2023-01.parquet\")\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .saveAsTable(\"default.nyc_taxi\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c83220eb-0589-4b95-b52e-c3d2892c5fd6",
   "metadata": {},
   "source": [
    "## Schema Evolution: Handle Automatically\n",
    "If you trust the upstream data source (provider) then you can add the `option(\"mergeSchema\", \"true\")`. Otherwise, it is better to specifically select a subset of the columns you expected to see. In this example use case, the only known column is the `VendorID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c202293-8be8-45e5-bb42-617fb57ee6dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evolve the Schema. (Showcases how to auto-merge changes to the schema)\n",
    "# note: if you can trust the upstream, then this option is perfectly fine\n",
    "# however, if you don't trust the upstream, then it is good to opt-in to the \n",
    "# changing columns.\n",
    "\n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/data/datasets/nyc_taxi/yellow_tripdata_2023-01.parquet\")\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .option(\"mergeSchema\", \"true\")\n",
    "      .saveAsTable(\"default.nyc_taxi\")\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc5defff",
   "metadata": {},
   "source": [
    "# Alternatives to Auto Schema Evolution\n",
    "In the previous case, we used `.option(\"mergeSchema\", \"true\")` to modify the behavior of the Delta Lake writer. While this option simplifies how we evolve our Delta Lake table schemas, it comes at the price of not being fully aware of the changes to our table schema. In the case where there are unknown columns being introduced from an upstream source, you'll want to know which columns are intended to bring forward, and which columns can be safely ignored.\n",
    "\n",
    "## Intentionally Adding Columns with Alter Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb3df0d-7083-4bb7-8eda-8c64cd4b8f20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# manually set the columns. This is an example of intentional opt-in to the new columns outside of '.option(\"mergeSchema\", \"true\")`. \n",
    "# Note: this can be run once, afterwards the ADD columns will fail since they already exist\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE default.nyc_taxi \n",
    "ADD columns (\n",
    "  tpep_pickup_datetime TIMESTAMP,\n",
    "  tpep_dropoff_datetime TIMESTAMP,\n",
    "  passenger_count DOUBLE,\n",
    "  trip_distance DOUBLE,\n",
    "  RatecodeID DOUBLE,\n",
    "  store_and_fwd_flag STRING,\n",
    "  PULocationID BIGINT,\n",
    "  DOLocationID BIGINT,\n",
    "  payment_type BIGINT,\n",
    "  fare_amount DOUBLE,\n",
    "  extra DOUBLE,\n",
    "  mta_tax DOUBLE,\n",
    "  tip_amount DOUBLE,\n",
    "  tolls_amount DOUBLE,\n",
    "  improvement_surcharge DOUBLE,\n",
    "  total_amount DOUBLE,\n",
    "  congestion_surcharge DOUBLE, \n",
    "  airport_fee DOUBLE\n",
    ");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0e41d-5a7b-4c4c-82a5-7e14d230204e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view the table structure using DESCRIBE\n",
    "spark.sql(\"describe extended default.nyc_taxi\").show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde40ae-604b-453c-bb25-4fb6592410f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from default.nyc_taxi limit 10\").show(truncate=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4244ff8f-89e4-4aaf-8e5c-3363a58ec4f0",
   "metadata": {},
   "source": [
    "# Adding and Modifying Table Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a5b346-e518-41bf-a16f-021772e020e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE default.nyc_taxi \n",
    "SET TBLPROPERTIES (\n",
    "  'delta.logRetentionDuration'='interval 14 days',\n",
    "  'delta.deletedFileRetentionDuration'='interval 28 days'\n",
    ")\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c2661-09e9-4798-a2e4-8181d5215213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view the Table History\n",
    "# this will show the SET TBLPROPERTIES transaction in the Delta Lake transaction log\n",
    "dt = DeltaTable.forName(spark, 'default.nyc_taxi')\n",
    "(\n",
    "    dt\n",
    "    .history(10)\n",
    "    .select(\"version\", \"timestamp\", \"operation\")\n",
    "    .show(truncate=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d9a0fe-b95a-4f32-bf50-730b0b5207d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view your Delta Lake tblproperties using Spark SQL\n",
    "# you'll see the tblproperties we added as well as the delta.minReaderVersion, delta.minWriterVersion\n",
    "spark.sql(\"show tblproperties default.nyc_taxi\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d59ac-4ca3-4c36-9282-cd8446be8dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing the properties using the DeltaTable command\n",
    "(\n",
    "    DeltaTable\n",
    "    .forName(spark, 'default.nyc_taxi')\n",
    "    .detail()\n",
    "    .select(\"properties\")\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "513ba79a-e127-4e3f-937d-20073155bb60",
   "metadata": {},
   "source": [
    "# Removing Table Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b84468-0f34-4996-8595-f84c2f1fc907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using unset to remove a property.\n",
    "# in the case of a key not existing, the operation becomes noop\n",
    "# so there is no need to use IF EXISTS conditionals\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE default.nyc_taxi\n",
    "    UNSET TBLPROPERTIES('delta.loRgetentionDuratio')\n",
    "  \"\"\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b42f760-7124-4be3-8a0b-0b2d3f5e4159",
   "metadata": {},
   "source": [
    "# Delta Table Optimizations (Cleaning, Tuning)\n",
    "> The following section showcases how to create and fix a poorly optimized Delta Lake table\n",
    "* Long Running Code will have a warning before hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107377e1-990c-4ad5-ad28-a9d4d8abc3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new table called `nonoptimal_nyc_taxi` in the `default` database.\n",
    "(DeltaTable.createIfNotExists(spark)\n",
    "    .tableName(\"default.nonoptimal_nyc_taxi\")\n",
    "    .property(\"description\", \"table to be optimized\")\n",
    "    .addColumn(\"VendorID\", \"BIGINT\")\n",
    "    .addColumn(\"tpep_pickup_datetime\", \"TIMESTAMP\")\n",
    "    .addColumn(\"tpep_dropoff_datetime\", \"TIMESTAMP\")\n",
    "    .addColumn(\"passenger_count\", \"DOUBLE\")\n",
    "    .addColumn(\"trip_distance\", \"DOUBLE\")\n",
    "    .addColumn(\"RatecodeID\", \"DOUBLE\")\n",
    "    .addColumn(\"store_and_fwd_flag\", \"STRING\")\n",
    "    .addColumn(\"PULocationID\", \"BIGINT\")\n",
    "    .addColumn(\"DOLocationID\", \"BIGINT\")\n",
    "    .addColumn(\"payment_type\", \"BIGINT\")\n",
    "    .addColumn(\"fare_amount\", \"DOUBLE\")\n",
    "    .addColumn(\"extra\", \"DOUBLE\")\n",
    "    .addColumn(\"mta_tax\", \"DOUBLE\")\n",
    "    .addColumn(\"tip_amount\", \"DOUBLE\")\n",
    "    .addColumn(\"tolls_amount\", \"DOUBLE\")\n",
    "    .addColumn(\"improvement_surcharge\", \"DOUBLE\")\n",
    "    .addColumn(\"total_amount\", \"DOUBLE\")\n",
    "    .addColumn(\"congestion_surcharge\", \"DOUBLE\")\n",
    "    .addColumn(\"airport_fee\", \"DOUBLE\")\n",
    "    .execute()\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a90c08-4c10-45a7-88b3-34b0c9567ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9203909a-881e-4245-a205-c79d2bfa7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to take a Row and save as a separate Table\n",
    "# note: this is for example, this is not optimal\n",
    "# note 2: if you want to use something similar, please use `rows` to write a collection of rows per transaction\n",
    "def append_row_to_table(row, schema, table):\n",
    "    (spark.createDataFrame([row], schema)\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .saveAsTable(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4791275-2ce9-4e45-a010-fb1fceae477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Warning: will run for a while\n",
    "source_df = spark.table(\"default.nyc_taxi\")\n",
    "source_schema = source_df.schema\n",
    "destination_table = \"default.nonoptimal_nyc_taxi\"\n",
    "# change limit to 1000 or 10000 if you want to generate a more verbose example\n",
    "limit = 100\n",
    "\n",
    "# warning: list comprehension is used here to run synchronous inserts and to prove a point\n",
    "# please don't use this code for production use cases, and just to create a poorly optimized table\n",
    "([\n",
    "    append_row_to_table(row, source_schema, destination_table) \n",
    "    for row in source_df.limit(limit).collect()\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47d58725-bd63-4860-9470-309ca5f533c2",
   "metadata": {},
   "source": [
    "## Using Optimize\n",
    "> Using Bin-Packing Optimize (default) will allow us to coalesce many small files (which we just created) into fewer large files.\n",
    "\n",
    "Spark Config:\n",
    "1. `spark.databricks.delta.optimize.repartition.enabled=true` is useful when we have many small files like we do in the case of the nonoptimal_nyc_taxi Delta Lake Table.\n",
    "\n",
    "Databricks-Only: Delta Lake Table Properties:\n",
    "1. `delta.targetFileSize=20mb`\n",
    "2. `delta.tuneFileSizesForRewrites=true`\n",
    "\n",
    "When running OPTIMIZE outside of databricks, like we are inside this jupyter notebook, we can lean on some alternative spark configuration to control how many files we read and how we can optimize differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fca41c3-f010-4110-ac23-9885ca7e2229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the table properties for OPTIMIZE\n",
    "\n",
    "# note: these configurations are only for databricks at the time of writing so the \n",
    "# `spark.databricks.delta.allowArbitraryProperties.enabled` is used to prevent an exception from being thrown\n",
    "spark.conf.set('spark.databricks.delta.allowArbitraryProperties.enabled','true')\n",
    "spark.conf.set('spark.databricks.delta.optimize.repartition.enabled', 'true')\n",
    "spark.conf.set('spark.sql.files.maxRecordsPerFile', '1000000')\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE default.nonoptimal_nyc_taxi\n",
    "    SET TBLPROPERTIES (\n",
    "      'delta.targetFileSize'='20mb',\n",
    "      'delta.tuneFileSizesForRewrites'='true'\n",
    "    )\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b390c1d-a8f4-4189-9e12-c541dc758cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute OPTIMIZE from the Delta python client\n",
    "df = (\n",
    "    DeltaTable.forName(spark, \"default.nonoptimal_nyc_taxi\")\n",
    "    .optimize()\n",
    "    .executeCompaction()\n",
    ")\n",
    "df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d2d9b-4719-4b39-8382-6e784ce8bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the full metadata from the OPTIMIZE operation\n",
    "\n",
    "(\n",
    "    DeltaTable\n",
    "    .forName(spark, \"default.nonoptimal_nyc_taxi\")\n",
    "    .history(2)\n",
    "    .where(col(\"operation\") == \"OPTIMIZE\")\n",
    "    .select(\"version\", \"timestamp\", \"operation\", \"operationMetrics.numRemovedFiles\", \"operationMetrics.numAddedFiles\")\n",
    "    .show(truncate=False)\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42434366-d128-46be-b579-c866bdde76a6",
   "metadata": {},
   "source": [
    "# Using Z-Order Optimize\n",
    "> Z-Order optimize is a co-location technique to minimize the total number of files loaded to answer common questions (queries) from your Delta Lake tables. For example, let’s say 80% of the queries to the `nyc_taxi dataset` always search first for `tpep_pickup_datetime` followed by a specific `RatecodeID`. You could optimize for faster query results by co-locating the `tpep_pickup_datetime` and `RatecodeID` so that the search space is reduced\n",
    "> This allows us to reduce the number of files that need to be opened, using data skipping, since Delta Lake captures statistics automatically for the first 32 columns of a Delta Lake table.\n",
    "\n",
    "## Delta Lake Table Properties\n",
    "`delta.dataSkippingNumIndexedCols=6` could be used in the case where we only care about the first 6 columns of our Delta Lake table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfcbd0c-5ebb-4d68-9d99-a76804183a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the statistics collected for the table from the default 32 down to 6\n",
    "# since we will be calling zorder optimize, this setting can take effect\n",
    "# along with the operation itself\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE default.nonoptimal_nyc_taxi\n",
    "    SET TBLPROPERTIES (\n",
    "      'delta.dataSkippingNumIndexedCols'='6'\n",
    "    )\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05de0efe-917b-47a1-82bf-3b5dc5498fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://docs.databricks.com/delta/data-skipping.html for more details\n",
    "dt = DeltaTable.forName(spark, \"default.nonoptimal_nyc_taxi\")\n",
    "(\n",
    "    dt\n",
    "    .optimize()\n",
    "    .executeZOrderBy(\"tpep_pickup_datetime\", \"RatecodeID\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470ded9a-328b-4d6f-83ab-b6eecaedbdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the results of the optimization\n",
    "# in the case where we user z-order optimize on a single file, it isn't going to help much\n",
    "# but you get the idea!\n",
    "(\n",
    "    dt.history(10)\n",
    "    .where(col(\"operation\") == \"OPTIMIZE\")\n",
    "    .select(\"version\", \"timestamp\", \"operation\", \"operationMetrics.numRemovedFiles\", \"operationMetrics.numAddedFiles\")\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e589dfb5-2194-43fc-86ab-0585fddb3127",
   "metadata": {},
   "source": [
    "# Partition Tuning\n",
    "> Note: for tables under 1TB it isn't advised to use any partitioning and lean on OPTIMIZE and Z-ORDER OPTIMIZE.\n",
    "> Edge Cases: GDPR and Data Governance: N day TTLs (30, 10, and 7 day policies are fairly standard for TTL)\n",
    "\n",
    "You'll learn to achieve the following next:\n",
    "1. Create a Table Partitioned by a given Column\n",
    "2. Add or Remove Partitions for a given table\n",
    "3. Modify an existing, non-partitioned table, to introduce partitioning (* requires some communication to your downstream data consumers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caf2c2cf-6d31-4a5e-80d2-149a500c1017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to go back and drop this table. \n",
    "# spark.sql(\"drop table default.nyc_taxi_by_day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f95f9b0-504e-40a2-b1b7-3a72ab75b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table Creation with Partitions\n",
    "(DeltaTable.createIfNotExists(spark)\n",
    "  .tableName(\"default.nyc_taxi_by_day\")\n",
    "  .addColumn(\"VendorID\", \"BIGINT\")\n",
    "  .addColumn(\"tpep_pickup_datetime\", \"TIMESTAMP\")\n",
    "  .addColumn(\"tpep_dropoff_datetime\", \"TIMESTAMP\", comment=\"trip drop off and partition source column\")\n",
    "  .addColumn(\"tpep_dropoff_date\", DateType(), generatedAlwaysAs=\"CAST(tpep_dropoff_datetime AS DATE)\")\n",
    "  .addColumn(\"passenger_count\", \"DOUBLE\")\n",
    "  .addColumn(\"trip_distance\", \"DOUBLE\")\n",
    "  .addColumn(\"RatecodeID\", \"DOUBLE\")\n",
    "  .addColumn(\"store_and_fwd_flag\", \"STRING\")\n",
    "  .addColumn(\"PULocationID\", \"BIGINT\")\n",
    "  .addColumn(\"DOLocationID\", \"BIGINT\")\n",
    "  .addColumn(\"payment_type\", \"BIGINT\")\n",
    "  .addColumn(\"fare_amount\", \"DOUBLE\")\n",
    "  .addColumn(\"extra\", \"DOUBLE\")\n",
    "  .addColumn(\"mta_tax\", \"DOUBLE\")\n",
    "  .addColumn(\"tip_amount\", \"DOUBLE\")\n",
    "  .addColumn(\"tolls_amount\", \"DOUBLE\")\n",
    "  .addColumn(\"improvement_surcharge\", \"DOUBLE\")\n",
    "  .addColumn(\"total_amount\", \"DOUBLE\")\n",
    "  .addColumn(\"congestion_surcharge\", \"DOUBLE\")\n",
    "  .addColumn(\"airport_fee\", \"DOUBLE\")\n",
    "  .partitionedBy(\"tpep_dropoff_date\")\n",
    "  .property(\"description\", \"partitioned by taxi trip end time\")\n",
    "  .property(\"delta.logRetentionDuration\", \"interval 30 days\")\n",
    "  .property(\"delta.deletedFileRetentionDuration\", \"interval 1 day\")\n",
    "  .property(\"delta.dataSkippingNumIndexedCols\", \"10\")\n",
    "  .property(\"delta.checkpoint.writeStatsAsStruct\", \"true\")\n",
    "  .property(\"delta.checkpoint.writeStatsAsJson\", \"false\")\n",
    "  .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f11a2-c8ee-4529-b075-2ba23ea38e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from our default.nyc_taxi table into the newly created nyc_taxi_by_day table \n",
    "# this will allow us to generate a partitioned table\n",
    "# note: append will continue to add to the table, if you run this block multiple times - you'll have duplicates\n",
    "(\n",
    "    spark\n",
    "    .table(\"default.nyc_taxi\")\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"false\")\n",
    "    .saveAsTable(\"default.nyc_taxi_by_day\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d29c113-3b24-4246-a36b-0bfaa258969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"describe extended default.nyc_taxi_by_day\").show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df7a956a-c5d1-432f-8125-787ae2799a61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# removing bad data.\n",
    "# ls -la ch05/spark-warehouse/nyc_taxi_by_day/\n",
    "# will create some interesting table partitions: \n",
    "# tpep_dropoff_date=2009-01-01\n",
    "# tpep_dropoff_date=2022-10-24\n",
    "# tpep_dropoff_date=2022-10-25\n",
    "# tpep_dropoff_date=2022-12-31\n",
    "\n",
    "# we can investigate what is incorrect with the data in each of these partitions, or just wipe them out, as an exploration, \n",
    "# feel free to put your detectives hat on and see what is wrong with the data, or just delete the specific dates.\n",
    "(\n",
    "    DeltaTable\n",
    "    .forName(spark, 'default.nyc_taxi_by_day')\n",
    "    .delete(col(\"tpep_dropoff_date\") < \"2023-01-01\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c43bc52-8f25-4643-8088-1ddf8167c369",
   "metadata": {},
   "source": [
    "# Ensure the erroneous data is out of the current table Snapshot\n",
    "While we have deleted the bad partitions, we haven't wiped them out fully, to do so we will need to vacuum the table which we'll get to later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca27d4c1-d73e-4ac5-877f-e34a1ed36faa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_table = spark.table(\"default.nyc_taxi_by_day\").where(col(\"tpep_dropoff_date\") < \"2023-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec34338-104e-43c9-9982-021ac26a258f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c6673-b0c8-4cc7-9d4c-e012580dafcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the table itself has around 3 million trips\n",
    "spark.table(\"default.nyc_taxi_by_day\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c3326-d622-4676-b29a-6026b5d6d099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using time travel to \n",
    "dt = DeltaTable.forName(spark, 'default.nyc_taxi_by_day')\n",
    "#dt.detail().printSchema()\n",
    "\"\"\"\n",
    "root\n",
    " |-- format: string (nullable = true)\n",
    " |-- id: string (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    " |-- description: string (nullable = true)\n",
    " |-- location: string (nullable = true)\n",
    " |-- createdAt: timestamp (nullable = true)\n",
    " |-- lastModified: timestamp (nullable = true)\n",
    " |-- partitionColumns: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    " |-- numFiles: long (nullable = true)\n",
    " |-- sizeInBytes: long (nullable = true)\n",
    " |-- properties: map (nullable = true)\n",
    " |    |-- key: string\n",
    " |    |-- value: string (valueContainsNull = true)\n",
    " |-- minReaderVersion: integer (nullable = true)\n",
    " |-- minWriterVersion: integer (nullable = true)\n",
    " |-- tableFeatures: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    "\"\"\"\n",
    "\n",
    "# view the table details\n",
    "dt.detail().select(\"partitionColumns\", \"numFiles\", \"lastModified\").show(truncate=False)\n",
    "\n",
    "# describe the extended history of the table\n",
    "spark.sql(\"describe extended default.nyc_taxi_by_day\").show(40, truncate=True)\n",
    "\n",
    "# gather partition days\n",
    "from pyspark.sql.functions import desc\n",
    "(spark\n",
    "   .table('default.nyc_taxi_by_day')\n",
    "   .select(\"tpep_dropoff_date\")\n",
    "   .distinct()\n",
    "   .sort(desc(\"tpep_dropoff_date\"))\n",
    "   .show(33))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6485276f-73c9-4fb6-a3c9-40d39d4c8ea1",
   "metadata": {},
   "source": [
    "# Recovering Data using Partitions using \"Replays\"\n",
    "> Let's face it. Even with the best intentions in place, we are all human and make mistakes. In your career as a data engineer, one thing you'll be required to learn is the art of data recovery. When we recover data, the process is commonly called 'replaying' since the action we are taking is to rollback the clock, or rewind, to an earlier point in time. This enables us to remove problematic changes to a table, and replace the erroneous data with whatever the \"fixed\" data is.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc5c5fe-1234-4c4b-b101-d5f379e869e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using write mode 'overwrite' and replaceWhere condition\n",
    "recovery_table = \"\"\n",
    "partition_col = \"\"\n",
    "table_to_fix = \"\"\n",
    "(\n",
    "  spark\n",
    "    .table(recovery_table)\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"replaceWhere\", f\"{partition_col} == '2023-01-01'\")\n",
    "    .saveAsTable(table_to_fix)\n",
    ")    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
